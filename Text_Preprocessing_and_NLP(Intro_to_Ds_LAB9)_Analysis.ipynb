{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0x5DYgyOSE0+W0/W6i7ZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmad3752/Colab/blob/main/Text_Preprocessing_and_NLP(Intro_to_Ds_LAB9)_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYuebl-Q63DX",
        "outputId": "491901ed-a6f1-4301-9c6b-dd8f1d5960a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnlxWykI8KiF",
        "outputId": "16847b56-ca89-4d66-e277-207f63ad4ae4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPClkdKK8Mzr",
        "outputId": "1c1fcb9e-1142-4cad-e8d2-54c6c226b155"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "fil='/content/simple_nlp_text.pdf'\n",
        "with open(fil,'rb') as pdfFileobj:\n",
        "  pdfReader = PyPDF2.PdfReader(pdfFileobj)\n",
        "  pages=len(pdfReader.pages)\n",
        "  print(f\" the no of pages are : {pages}\")\n",
        "  text = \"\"\n",
        "  for page in pdfReader.pages:\n",
        "        text += page.extract_text()\n",
        "print(\"The first 500 charachters are \")\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO5NzFpj8S3V",
        "outputId": "d8cc4724-42ed-4d14-8cf6-9f348bff9e2a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the no of pages are : 3\n",
            "Page 1:\n",
            "Natural Language Processing (NLP) is a branch of artificial intelligence that deals with the\n",
            "interaction between computers and humans using natural language.Page 2:\n",
            "The goal of NLP is to enable computers to understand, interpret, and generate human language in a\n",
            "way that is both meaningful and useful.Page 3:\n",
            "Common tasks in NLP include text classification, sentiment analysis, machine translation, and\n",
            "question answering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqXRXPlM9Nx8",
        "outputId": "5493acc3-4170-4871-dd39-0811ad5da1d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTuJ78NeA5Wh",
        "outputId": "698e6f1c-9157-4d2a-aa8a-738c537f7654"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "word_tok=word_tokenize(text)\n",
        "sent_tok=sent_tokenize(text)\n",
        "\n",
        "\n",
        "print(\"Words:\", word_tok)\n",
        "print(\"Sentences:\", sent_tok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHbKajJK--Pe",
        "outputId": "93a0d6e2-c092-480f-a9e0-a980fee149d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Page', '1', ':', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language.Page', '2', ':', 'The', 'goal', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'useful.Page', '3', ':', 'Common', 'tasks', 'in', 'NLP', 'include', 'text', 'classification', ',', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'and', 'question', 'answering', '.']\n",
            "Sentences: ['Page 1:\\nNatural Language Processing (NLP) is a branch of artificial intelligence that deals with the\\ninteraction between computers and humans using natural language.Page 2:\\nThe goal of NLP is to enable computers to understand, interpret, and generate human language in a\\nway that is both meaningful and useful.Page 3:\\nCommon tasks in NLP include text classification, sentiment analysis, machine translation, and\\nquestion answering.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"the len of words before ; {len(word_tok)}\")\n",
        "words=[word for word in word_tok if word.isalpha()]\n",
        "print(f\"the len of words before ; {len(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr0rHX7O_WwN",
        "outputId": "f0e0e262-55ea-4b75-f540-5d12b17486ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the len of words before ; 75\n",
            "the len of words before ; 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "_UsOUMvYCCcp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_fdist = FreqDist()\n",
        "\n",
        "for word in words:\n",
        "  text_fdist[word.lower()]+=1\n",
        "\n",
        "text_fdist\n",
        "top_10_words = text_fdist.most_common(10)\n",
        "\n",
        "print(\"Top 10 Most Frequent Words:\")\n",
        "for word, freq in top_10_words:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ9Ritc0CfSs",
        "outputId": "0b1d1f11-7e38-461c-cc5f-47a655446511"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Frequent Words:\n",
            "and: 4\n",
            "nlp: 3\n",
            "is: 3\n",
            "natural: 2\n",
            "language: 2\n",
            "a: 2\n",
            "of: 2\n",
            "that: 2\n",
            "the: 2\n",
            "computers: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BIGRAMS,TRIGAMS,NGRAMS\n",
        "from nltk.util import bigrams, trigrams, ngrams\n",
        "text_wtok_bigram = list(bigrams(words))\n",
        "bigram_fdict=FreqDist(text_wtok_bigram)\n",
        "top_10_bigrams=bigram_fdict.most_common(10)\n",
        "for bigram, freq in top_10_bigrams:\n",
        "    print(f\"{bigram}: {freq}\")\n",
        "\n",
        "text_wtok_trigram = list(trigrams(words))\n",
        "trigram_fdist = FreqDist(text_wtok_trigram)\n",
        "top_10_trigrams = trigram_fdist.most_common(10)\n",
        "\n",
        "print(\"\\nTop 10 Most Frequent Trigrams:\")\n",
        "for trigram, freq in top_10_trigrams:\n",
        "    print(f\"{trigram}: {freq}\")\n",
        "\n",
        "n = 4\n",
        "text_wtok_ngram = list(ngrams(words, n))\n",
        "ngram_fdist = FreqDist(text_wtok_ngram)\n",
        "top_10_ngrams = ngram_fdist.most_common(10)\n",
        "\n",
        "print(f\"\\nTop 10 Most Frequent {n}-grams:\")\n",
        "for ngram, freq in top_10_ngrams:\n",
        "    print(f\"{ngram}: {freq}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1A0HiiXCtn7",
        "outputId": "3dbe5684-271e-4958-bc59-fb5d9198cdaf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('NLP', 'is'): 2\n",
            "('Page', 'Natural'): 1\n",
            "('Natural', 'Language'): 1\n",
            "('Language', 'Processing'): 1\n",
            "('Processing', 'NLP'): 1\n",
            "('is', 'a'): 1\n",
            "('a', 'branch'): 1\n",
            "('branch', 'of'): 1\n",
            "('of', 'artificial'): 1\n",
            "('artificial', 'intelligence'): 1\n",
            "\n",
            "Top 10 Most Frequent Trigrams:\n",
            "('Page', 'Natural', 'Language'): 1\n",
            "('Natural', 'Language', 'Processing'): 1\n",
            "('Language', 'Processing', 'NLP'): 1\n",
            "('Processing', 'NLP', 'is'): 1\n",
            "('NLP', 'is', 'a'): 1\n",
            "('is', 'a', 'branch'): 1\n",
            "('a', 'branch', 'of'): 1\n",
            "('branch', 'of', 'artificial'): 1\n",
            "('of', 'artificial', 'intelligence'): 1\n",
            "('artificial', 'intelligence', 'that'): 1\n",
            "\n",
            "Top 10 Most Frequent 4-grams:\n",
            "('Page', 'Natural', 'Language', 'Processing'): 1\n",
            "('Natural', 'Language', 'Processing', 'NLP'): 1\n",
            "('Language', 'Processing', 'NLP', 'is'): 1\n",
            "('Processing', 'NLP', 'is', 'a'): 1\n",
            "('NLP', 'is', 'a', 'branch'): 1\n",
            "('is', 'a', 'branch', 'of'): 1\n",
            "('a', 'branch', 'of', 'artificial'): 1\n",
            "('branch', 'of', 'artificial', 'intelligence'): 1\n",
            "('of', 'artificial', 'intelligence', 'that'): 1\n",
            "('artificial', 'intelligence', 'that', 'deals'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
      ],
      "metadata": {
        "id": "GpkpsOigFy67"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "•\tPorter: Less aggressive (e.g., \"happily\" → \"happili\").\n",
        "•\tLancaster: More aggressive (e.g., \"happily\" → \"happy\").\n",
        "•\tSnowball: Supports multiple languages.\n",
        "\"\"\"\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "porter_stemmed = [porter_stemmer.stem(word) for word in words]\n",
        "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in words]\n",
        "snowball_stemmed = [snowball_stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original Words \")\n",
        "print(words)\n",
        "\n",
        "print(\"\\nPorter Stemmer Results:\")\n",
        "print(porter_stemmed)\n",
        "\n",
        "print(\"\\nLancaster Stemmer Results:\")\n",
        "print(lancaster_stemmed)\n",
        "\n",
        "print(\"\\nSnowball Stemmer Results:\")\n",
        "print(snowball_stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v4oS0O1INQL",
        "outputId": "81e9c5ab-f41a-4973-b901-5b600d845396"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words \n",
            "['Page', 'Natural', 'Language', 'Processing', 'NLP', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'The', 'goal', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'Common', 'tasks', 'in', 'NLP', 'include', 'text', 'classification', 'sentiment', 'analysis', 'machine', 'translation', 'and', 'question', 'answering']\n",
            "\n",
            "Porter Stemmer Results:\n",
            "['page', 'natur', 'languag', 'process', 'nlp', 'is', 'a', 'branch', 'of', 'artifici', 'intellig', 'that', 'deal', 'with', 'the', 'interact', 'between', 'comput', 'and', 'human', 'use', 'natur', 'the', 'goal', 'of', 'nlp', 'is', 'to', 'enabl', 'comput', 'to', 'understand', 'interpret', 'and', 'gener', 'human', 'languag', 'in', 'a', 'way', 'that', 'is', 'both', 'meaning', 'and', 'common', 'task', 'in', 'nlp', 'includ', 'text', 'classif', 'sentiment', 'analysi', 'machin', 'translat', 'and', 'question', 'answer']\n",
            "\n",
            "Lancaster Stemmer Results:\n",
            "['pag', 'nat', 'langu', 'process', 'nlp', 'is', 'a', 'branch', 'of', 'art', 'intellig', 'that', 'deal', 'with', 'the', 'interact', 'between', 'comput', 'and', 'hum', 'us', 'nat', 'the', 'goal', 'of', 'nlp', 'is', 'to', 'en', 'comput', 'to', 'understand', 'interpret', 'and', 'gen', 'hum', 'langu', 'in', 'a', 'way', 'that', 'is', 'both', 'mean', 'and', 'common', 'task', 'in', 'nlp', 'includ', 'text', 'class', 'senty', 'analys', 'machin', 'transl', 'and', 'quest', 'answ']\n",
            "\n",
            "Snowball Stemmer Results:\n",
            "['page', 'natur', 'languag', 'process', 'nlp', 'is', 'a', 'branch', 'of', 'artifici', 'intellig', 'that', 'deal', 'with', 'the', 'interact', 'between', 'comput', 'and', 'human', 'use', 'natur', 'the', 'goal', 'of', 'nlp', 'is', 'to', 'enabl', 'comput', 'to', 'understand', 'interpret', 'and', 'generat', 'human', 'languag', 'in', 'a', 'way', 'that', 'is', 'both', 'meaning', 'and', 'common', 'task', 'in', 'nlp', 'includ', 'text', 'classif', 'sentiment', 'analysi', 'machin', 'translat', 'and', 'question', 'answer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "vNybcC5wI3oS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6a0Q6UQJ2YK",
        "outputId": "85c0206d-39ba-4976-fbf6-4a0341e8d9d0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatized_words=[lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original Words \")\n",
        "print(words)\n",
        "print(\"Lemmatized Words \")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gotfz0iqKBaL",
        "outputId": "bf064a42-9513-4654-95e9-b569fdc889f1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words \n",
            "['Page', 'Natural', 'Language', 'Processing', 'NLP', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'The', 'goal', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'Common', 'tasks', 'in', 'NLP', 'include', 'text', 'classification', 'sentiment', 'analysis', 'machine', 'translation', 'and', 'question', 'answering']\n",
            "Lemmatized Words \n",
            "['Page', 'Natural', 'Language', 'Processing', 'NLP', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'deal', 'with', 'the', 'interaction', 'between', 'computer', 'and', 'human', 'using', 'natural', 'The', 'goal', 'of', 'NLP', 'is', 'to', 'enable', 'computer', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'Common', 'task', 'in', 'NLP', 'include', 'text', 'classification', 'sentiment', 'analysis', 'machine', 'translation', 'and', 'question', 'answering']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "h_GxhaCWKNtc"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "freq_dist=FreqDist(filtered_words)\n",
        "top_10_words=freq_dist.most_common(10)\n",
        "print(\"Top 10 Most Frequent Words After Stopword Removal:\")\n",
        "for word, freq in top_10_words:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4HP_0OoK0c8",
        "outputId": "a39648e2-117a-4d49-f2cb-c827afe7c05b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Frequent Words After Stopword Removal:\n",
            "NLP: 3\n",
            "computers: 2\n",
            "Page: 1\n",
            "Natural: 1\n",
            "Language: 1\n",
            "Processing: 1\n",
            "branch: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "deals: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPOFw3gSK6GX",
        "outputId": "c7c1a56a-2115-4487-cea5-549d859c933a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_words = pos_tag(filtered_words)\n",
        "print(\"POS Tags for Each Word:\")\n",
        "for word, tag in tagged_words:\n",
        "    print(f\"{word}: {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFqHyl9GLrB6",
        "outputId": "743c08d8-2e0d-42c3-b949-f25e80dfcd74"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags for Each Word:\n",
            "Page: NNP\n",
            "Natural: NNP\n",
            "Language: NNP\n",
            "Processing: NNP\n",
            "NLP: NNP\n",
            "branch: NN\n",
            "artificial: JJ\n",
            "intelligence: NN\n",
            "deals: NNS\n",
            "interaction: VBP\n",
            "computers: NNS\n",
            "humans: NNS\n",
            "using: VBG\n",
            "natural: JJ\n",
            "goal: NN\n",
            "NLP: NNP\n",
            "enable: JJ\n",
            "computers: NNS\n",
            "understand: VBP\n",
            "interpret: JJ\n",
            "generate: NN\n",
            "human: JJ\n",
            "language: NN\n",
            "way: NN\n",
            "meaningful: JJ\n",
            "Common: NNP\n",
            "tasks: NNS\n",
            "NLP: NNP\n",
            "include: VBP\n",
            "text: JJ\n",
            "classification: NN\n",
            "sentiment: NN\n",
            "analysis: NN\n",
            "machine: NN\n",
            "translation: NN\n",
            "question: NN\n",
            "answering: VBG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDKqcex6LwmJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}